configfile: "config/config.yaml"

rule all:
    input:
        "resources/bac120_taxonomy_r{}.tsv".format(config["gtdb_version"]),
        "resources/{}/selected_proteomes".format(config["dataset"]),
        "results/{}/genome2abbrev.csv".format(config["dataset"]),
        expand("results/{dataset}/mcl_clusters_I{inf}.txt", dataset=config["dataset"], inf=config["mcl"]["inflation"]),
        "results/{}/eggnog/annotations.emapper.annotations".format(config["dataset"]),
        expand("results/{dataset}/mcl/{inf}", dataset=config["dataset"], inf=config["mcl"]["inflation"])

rule get_all_proteomes:
    output:
        "resources/all_proteomes.tar.gz",
    shell:
        "wget '{config[gtdb_base_url]}release{config[gtdb_version]}/{config[gtdb_version]}.{config[gtdb_subversion]}/genomic_files_reps/gtdb_proteins_aa_reps_r{config[gtdb_version]}.tar.gz' -O {output}"

rule get_all_genomes:
    output:
        "resources/all_genomes.tar.gz"
    shell:
        "wget '{config[gtdb_base_url]}release{config[gtdb_version]}/{config[gtdb_version]}.{config[gtdb_subversion]}/genomic_files_reps/gtdb_genomes_reps_r{params.gtdb_version}.tar.gz' -O {output}"

rule extract_all_proteomes:
    input:
        "resources/all_proteomes.tar.gz"
    output:
        directory(config["all_proteomes_outdir"])
    shell:
        """
        mkdir {config[all_proteomes_outdir]}
        tar -xzf {input} -C {config[all_proteomes_outdir]} --strip-components=1
        """

rule extract_all_genomes:
    input:
        "resources/all_genomes.tar.gz"
    output:
        directory(config["all_genomes_outdir"])
    shell:
        """
        mkdir {config[all_genomes_outdir]}
        tar -xzf {input} -C {config[all_genomes_outdir]} --strip-components=1
        """

rule select_proteomes:
    input:
        "resources/{dataset}/list_of_accession_ids.txt",
        "resources/all_proteomes"
    output:
        directory("resources/{dataset}/selected_proteomes")
    shell:
        "workflow/scripts/select_proteomes.sh {input[0]} {input[1]} {output}"

rule select_genomes:
    input:
        "resources/{dataset}/list_of_accession_ids.txt",
        "resources/all_genomes"
    output:
        directory("resources/{dataset}/selected_genomes")
    shell:
        "workflow/scripts/select_genomes.sh {input[0]} {input[1]} {output}"

rule get_bacterial_taxonomy_file:
    output:
        "resources/bac120_taxonomy_r{}.tsv.gz".format(config["gtdb_version"])
    shell:
        "wget '{config[gtdb_base_url]}release{config[gtdb_version]}/{config[gtdb_version]}.{config[gtdb_subversion]}/bac120_taxonomy_r{config[gtdb_version]}.tsv.gz' -O {output} || wget '{config[gtdb_base_url]}release{config[gtdb_version]}/{config[gtdb_version]}.{config[gtdb_subversion]}/bac_taxonomy_r{config[gtdb_version]}.tsv.gz' -O {output}"
	
rule unzip_bacterial_taxonomy_file:
    input:
        "resources/bac120_taxonomy_r{}.tsv.gz".format(config["gtdb_version"])
    output:
        "resources/bac120_taxonomy_r{}.tsv".format(config["gtdb_version"])
    shell:
        "gzip -d {input}"

rule create_genome2taxa:
    input:
        "resources/{dataset}/list_of_accession_ids.txt",
        "resources/bac120_taxonomy_r{}.tsv".format(config["gtdb_version"])
    output:
        "results/{dataset}/genome2taxa.csv"
    shell:
        """
        REGEXP=`tr '\\n' '|' < {input[0]} | sed 's/|$//'`
        grep -E "$REGEXP" {input[1]} | sed -E 's/\\t+/,/g' > {output}
        if [ -s "{config[manually_add_taxa_file]}" ]; then
             echo "Appending contents of file {config[manually_add_taxa_file]} to {output}"
             cat {config[manually_add_taxa_file]} >> {output}
        else
             echo "{config[manually_add_taxa_file]} not existing or empty"
        fi
        """

rule generate_shortcodes:
    input:
        "results/{dataset}/genome2taxa.csv"
    output:
        "results/{dataset}/genome2abbrev.csv"
    shell:
        "workflow/scripts/rename_genomes.py {input} {output}"

rule cat_all_sequences:
    input:
        "resources/{dataset}/selected_proteomes"
    output:
        "results/{dataset}/all_proteomes.faa"
    shell:
        "cat {input}/*.faa > {output}"

rule make_diamond_db:
    input:
        "results/{dataset}/all_proteomes.faa"
    output:
        "results/{dataset}/proteomes.dmnd"
    threads: config["diamond"]["threads"]
    shell:
        "diamond makedb --in {input} -d {output} --threads {threads}"

rule diamond_all_vs_all:
    input:
        db="results/{dataset}/proteomes.dmnd",
        query="results/{dataset}/all_proteomes.faa"
    output:
        "results/{dataset}/all_vs_all_wo_selfhits.tsv"
    resources:
        mem_mb_per_cpu=5000,
        # 1 day runtime
        runtime=1440
    threads: config["diamond"]["threads"]
    shell:
        """
        diamond blastp \
                --db     {input.db} \
                --query  {input.query} \
                --out    {output} \
                --outfmt 6 qseqid sseqid pident bitscore evalue qcovhsp scovhsp \
                --more-sensitive \
                --threads {threads}
        """

rule add_self_hits_to_diamond_all_vs_all:
    input:
        query="results/{dataset}/all_proteomes.faa",
        diamond="results/{dataset}/all_vs_all_wo_selfhits.tsv"
    output:
        "results/{dataset}/all_vs_all.tsv"
    shell:
        """
        cp {input.diamond} {output}
        # ---------- guarantee self-hits so singletons survive ---------- 
        # q s pident bits evalue qcov scov  (7 cols, tab-separated)
        awk '/^>/{{sub(/^>/,"",$0); printf "%s\\t%s\\t100\\t1000\\t1e-200\\t100\\t100\\n",$0,$0 }}' "{input.query}" >> "{output}"
        """


rule filter_and_convert_into_mcl_s_abc_format:
    input:
        "results/{dataset}/all_vs_all.tsv"
    output:
        "results/{dataset}/all_vs_all.abc"
    params:
        evalue_cut=config["diamond"]["evalue_cut"],
        cov_cut=config["diamond"]["cov_cut"]
    shell:
        "workflow/scripts/filter_and_convert.py -i {input} -o {output} -e {params.evalue_cut} -c {params.cov_cut}"

rule mcl_sweep:
    input:
        "results/{dataset}all_vs_all.abc"
    output:
        "results/{dataset}mcl_clusters_I{inflation}.txt"
    resources:
        mem_mb_per_cpu=config["mcl"]["memory"],
        runtime=config["mcl"]["runtime"]
    shell:
        "mcl {input} --abc -I {wildcards.inflation} -o {output}"

rule split_mcl_clusters:
    input:
        fasta="results/{dataset}/all_proteomes.faa",
        cluster="results/{dataset}/mcl_clusters_I{inflation}.txt"
        #cluster=expand("results/{dataset}/mcl_clusters_I{inf}.txt", dataset=config["dataset"], inf=config["mcl"]["inflation"])
    output:
        directory("results/{dataset}/mcl/{inflation}")
    params:
        huge_cutoff=300
    shell:
        "workflow/scripts/split_clusters.py -f {input.fasta} -c {params.huge_cutoff} -m {input.cluster} -o {output}"

rule download_eggnog_diamond_database:
    output:
        "resources/eggnog_diamond_db/eggnog.db",
        "resources/eggnog_diamond_db/eggnog_proteins.dmnd",
        "resources/eggnog_diamond_db/eggnog.taxa.db",
        "resources/eggnog_diamond_db/eggnog.taxa.db.traverse.pkl"
    params:
        outdir="resources/eggnog_diamond_db"
    shell:
        "mkdir -p {params.outdir}"
        "download_eggnog_data.py -y -f --data_dir {params.outdir}"

rule eggnog_mapper:
    input:
        fasta="results/{dataset}/all_proteomes.faa",
        db="resources/eggnog_diamond_db"
    output:
        "results/{dataset}/eggnog/annotations.emapper.annotations",
        "results/{dataset}/eggnog/annotations.emapper.hits",
        "results/{dataset}/eggnog/annotations.emapper.seed_orthologs",
    threads: config["eggnog"]["threads"]
    resources:
        mem_mb_per_cpu=4000,
        runtime=2880
    params:
        mode="diamond"
    shell:
        """
        emapper.py -i {input.fasta} --cpu {threads} --data_dir {input.db} -m {params.mode} --output {output} --override
        """

rule mafft_align:
    input:
        "results/{dataset}/mcl/{inflation}/Normal/{msa}.faa"
    output:
        "results/{dataset}/mafft/{inflation}/{msa}.aln"
    threads: config["mafft"]["threads"]
    resources:
        mem_mb_per_cpu=4000,
        runtime=600
    shell:
        "mafft --auto --thread {threads} {input} > {output}"

rule trimal_trim:
    input:
        "results/{dataset}/mafft/{inflation}/{msa}.aln"
    output:
        "results/{dataset}/trimal/{inflation}/{msa}.trim"
    resources:
        mem_mb_per_cpu=3000,
        runtime=600
    shell:
        "trimal -in {input} -out {output} -gt 0.1"

rule alignment_statistics:
    input:
        "results/{dataset}/trimal/{inflation}/"
    output:
        "results/{dataset}/alignment_statistics_I{inflation}.txt"
    shell:
        "workflow/scripts/alignment_stats.py {input} {output}"
